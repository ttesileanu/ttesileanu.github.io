---
layout: page
title: time series segmentation
description: segmenting time series using biologically plausible neural circuits
img: assets/img/project_icon_time_series.png
github: https://github.com/ttesileanu/bio-time-series
importance: 1
category: past
---

The information reaching our senses typically has a strong temporal component: the value
of the sound pressure at our ears at a given time is, for instance, strongly correlated
with the sound pressure at nearby moments. Beyond these short-time correlations, we also
expect longer-time contexts to be relevant. For example, the sort of short-time
correlations in sound pressure changes if the wind is howling and the AC is off, versus
if the AC is one, versus if it's broken.

<div class="row justify-content-sm-center">
    <div class="col-sm-10 mt-3 mt-md-0">
        {%- include figure.html path="assets/img/project_time_series_example_signal.png" dark_path="assets/img/project_time_series_example_signal-dark.png" title="Example segmented signal" class="img-fluid rounded" zoomable=true -%}
    </div>
</div>

A natural way of describing sensory data is as the output of a *dynamical system* acting
on stochastic inputs. The humming of cars on the highway, for instance, depends on the
actual traffic crossing the highway at each moment, which is a complicated signal whose
details we do not know and so it is naturally modelled as stochastic; however, the
vibrations generated by that traffic also follow the deterministic dynamics that governs
wave propagation in the road and in air. We would like to be able to detect changes in
this deterministic dynamics while ignoring the inevitable variability induced by the
stochastic inputs.

Mathematically, we can write down a model of the following form:

$$ y(t) = \sum_k z_k(t) \sum_i w_{ki} y(t - i) + \epsilon(t) \,, $$

where $$z_k(t)$$ is a one-hot encoding of context (i.e., one component $$z_{k^*}$$ is
equal to 1 while all the others are zero at any given time) and $$\epsilon(t)$$ is
white, Gaussian noise. For each $$k$$ we have an autoregressive (AR) process, while the
latent-state variable $$z_k(t)$$ indicates which of these processes is active at any
given time.

Our goal was to build an unsupervised learning algorithm that can segment the time
series $$y(t)$$ – that is, infer the AR coefficients $$w_{ki}$$, and find best estimates
for the latent-state assignments $$z_k(t)$$. We wanted to do this using a biologically
plausible neural implementation – one that works online, and in which synaptic
plasticity rules are local.

<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
        {%- include figure.html path="assets/img/project_time_series_circuits.png" dark_path="assets/img/project_time_series_circuits-dark.png" title="The model-based BioWTA circuit and the model-free autocorrelation circuit" class="img-fluid rounded" zoomable=true -%}
    </div>
</div>

We developed two different circuits capable of solving this time-series segmentation
task. One is model-based and basically amounts to a maximum likelihood estimate based on
the generative model written above. The second circuit is model-free and relies on
keeping a running estimate of autocorrelation coefficients, and then clustering these
coefficients using a biologically plausible circuit based on [non-negative similarity
matching](https://ieeexplore.ieee.org/abstract/document/7094519).

<div class="publications">
    <h2>publication</h2>
    {% bibliography -f papers -q @article[keywords ^= biowta] %}
</div>
